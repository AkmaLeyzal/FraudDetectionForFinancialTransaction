{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-fOPkp3Rq3a"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFRBr-5_RXOs",
        "outputId": "5a250af4-c5de-47c9-a33c-b3b18fe51fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrEGwsYdRuPy"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9v-lvuIFRW9g"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeNode:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gini = None\n",
        "        self.gain = None\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth: int, min_samples_split: int,\n",
        "                 class_weight: Optional[Dict[int, float]] = None,\n",
        "                 random_state: Optional[int] = None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.class_weight = class_weight\n",
        "        self.random_state = random_state\n",
        "        self.root = None\n",
        "        self.n_classes = None\n",
        "        self.feature_importances_ = None\n",
        "\n",
        "        if random_state is not None:\n",
        "            np.random.seed(random_state)\n",
        "\n",
        "    def _calculate_weighted_gini(self, y: np.ndarray) -> float:\n",
        "        if len(y) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        proportions = counts / len(y)\n",
        "\n",
        "        if self.class_weight is not None:\n",
        "            weights = np.array([self.class_weight.get(c, 1.0) for c in range(self.n_classes)])\n",
        "            proportions = proportions * weights\n",
        "            proportions = proportions / proportions.sum()\n",
        "\n",
        "        return 1.0 - np.sum(proportions ** 2)\n",
        "\n",
        "    def _split_data(self, X: np.ndarray, y: np.ndarray, feature: int,\n",
        "                    threshold: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        left_mask = X[:, feature] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        return (X[left_mask], X[right_mask],\n",
        "                y[left_mask], y[right_mask])\n",
        "\n",
        "    def _calculate_information_gain(self, y_parent: np.ndarray, y_left: np.ndarray,\n",
        "                                  y_right: np.ndarray) -> float:\n",
        "        n_left, n_right = len(y_left), len(y_right)\n",
        "        n_parent = len(y_parent)\n",
        "\n",
        "        if n_parent == 0:\n",
        "            return 0.0\n",
        "\n",
        "        gini_parent = self._calculate_weighted_gini(y_parent)\n",
        "        gini_left = self._calculate_weighted_gini(y_left)\n",
        "        gini_right = self._calculate_weighted_gini(y_right)\n",
        "\n",
        "        w_left = n_left / n_parent\n",
        "        w_right = n_right / n_parent\n",
        "\n",
        "        return gini_parent - (w_left * gini_left + w_right * gini_right)\n",
        "\n",
        "    def _find_best_split(self, X: np.ndarray, y: np.ndarray,\n",
        "                        feature_subset: Optional[List[int]] = None) -> Tuple[int, float, float]:\n",
        "        best_gain = -float('inf')\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        features = (feature_subset if feature_subset is not None\n",
        "                   else range(X.shape[1]))\n",
        "\n",
        "        for feature in features:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                X_left, X_right, y_left, y_right = self._split_data(\n",
        "                    X, y, feature, threshold\n",
        "                )\n",
        "\n",
        "                if len(y_left) == 0 or len(y_right) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = self._calculate_information_gain(y, y_left, y_right)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0,\n",
        "                    feature_subset: Optional[List[int]] = None) -> DecisionTreeNode:\n",
        "        n_samples = len(y)\n",
        "\n",
        "        if (depth >= self.max_depth or\n",
        "            n_samples < self.min_samples_split or\n",
        "            len(np.unique(y)) == 1):\n",
        "\n",
        "            if self.class_weight is not None:\n",
        "                counts = np.bincount(y, weights=[self.class_weight.get(yi, 1.0) for yi in y])\n",
        "            else:\n",
        "                counts = np.bincount(y)\n",
        "\n",
        "            node = DecisionTreeNode(value=np.argmax(counts))\n",
        "            node.gini = self._calculate_weighted_gini(y)\n",
        "            return node\n",
        "\n",
        "        best_feature, best_threshold, best_gain = self._find_best_split(\n",
        "            X, y, feature_subset\n",
        "        )\n",
        "\n",
        "        if best_feature is None:\n",
        "            node = DecisionTreeNode(value=Counter(y).most_common(1)[0][0])\n",
        "            node.gini = self._calculate_weighted_gini(y)\n",
        "            return node\n",
        "\n",
        "        X_left, X_right, y_left, y_right = self._split_data(\n",
        "            X, y, best_feature, best_threshold\n",
        "        )\n",
        "\n",
        "        node = DecisionTreeNode(\n",
        "            feature=best_feature,\n",
        "            threshold=best_threshold\n",
        "        )\n",
        "\n",
        "        node.gini = self._calculate_weighted_gini(y)\n",
        "        node.gain = best_gain\n",
        "\n",
        "        node.left = self._build_tree(X_left, y_left, depth + 1, feature_subset)\n",
        "        node.right = self._build_tree(X_right, y_right, depth + 1, feature_subset)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray,\n",
        "            feature_subset: Optional[List[int]] = None) -> 'DecisionTree':\n",
        "        y = y.astype(np.int32)\n",
        "        self.n_classes = len(np.unique(y))\n",
        "\n",
        "        if self.class_weight is None:\n",
        "            class_counts = np.bincount(y)\n",
        "            total = len(y)\n",
        "            self.class_weight = {\n",
        "                c: total / (self.n_classes * count)\n",
        "                for c, count in enumerate(class_counts)\n",
        "            }\n",
        "\n",
        "        self.root = self._build_tree(X, y, feature_subset=feature_subset)\n",
        "        self._calculate_feature_importance(X)\n",
        "        return self\n",
        "\n",
        "    def _traverse_tree(self, x: np.ndarray, node: DecisionTreeNode) -> int:\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _calculate_feature_importance(self, X: np.ndarray):\n",
        "        self.feature_importances_ = np.zeros(X.shape[1])\n",
        "        self._update_feature_importance(self.root, 1.0)\n",
        "\n",
        "        if np.sum(self.feature_importances_) > 0:\n",
        "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
        "\n",
        "    def _update_feature_importance(self, node: DecisionTreeNode, weight: float):\n",
        "        if node.feature is not None:\n",
        "            self.feature_importances_[node.feature] += weight * (node.gain or 0)\n",
        "            if node.left:\n",
        "                self._update_feature_importance(node.left, weight * 0.5)\n",
        "            if node.right:\n",
        "                self._update_feature_importance(node.right, weight * 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH88VO0NR1BD"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kjNseudjA3wp"
      },
      "outputs": [],
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_estimators: int, max_depth: int,\n",
        "                 min_samples_split: int, max_features: Union[str, float] = 'sqrt',\n",
        "                 random_state: Optional[int] = None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.trees = []\n",
        "        self.feature_importances_ = None\n",
        "        self.n_features = None\n",
        "\n",
        "        if random_state is not None:\n",
        "            np.random.seed(random_state)\n",
        "\n",
        "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    def _get_feature_subset_size(self, n_features: int) -> int:\n",
        "        if isinstance(self.max_features, str):\n",
        "            if self.max_features == 'sqrt':\n",
        "                return int(np.sqrt(n_features))\n",
        "            elif self.max_features == 'log2':\n",
        "                return int(np.log2(n_features))\n",
        "        elif isinstance(self.max_features, float):\n",
        "            return max(1, int(self.max_features * n_features))\n",
        "        return n_features\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.trees = []\n",
        "        n_features_subset = self._get_feature_subset_size(self.n_features)\n",
        "\n",
        "        print(f\"\\nTraining Random Forest with {self.n_estimators} trees...\")\n",
        "        print(f\"Features per tree: {n_features_subset}/{self.n_features}\")\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Training tree {i + 1}/{self.n_estimators}\")\n",
        "\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "\n",
        "            feature_subset = np.random.choice(\n",
        "                self.n_features,\n",
        "                size=n_features_subset,\n",
        "                replace=False\n",
        "            )\n",
        "\n",
        "            tree = DecisionTree(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                random_state=self.random_state + i if self.random_state else None\n",
        "            )\n",
        "\n",
        "            X_subset = X_sample[:, feature_subset]\n",
        "            tree.fit(X_subset, y_sample)\n",
        "\n",
        "            self.trees.append((tree, feature_subset))\n",
        "\n",
        "        self._calculate_feature_importance()\n",
        "        return self\n",
        "\n",
        "    def _calculate_feature_importance(self):\n",
        "        importances = np.zeros(self.n_features)\n",
        "        counts = np.zeros(self.n_features)\n",
        "\n",
        "        for tree, feature_subset in self.trees:\n",
        "            for idx, feature in enumerate(feature_subset):\n",
        "                if tree.feature_importances_ is not None:\n",
        "                    importances[feature] += tree.feature_importances_[idx]\n",
        "                    counts[feature] += 1\n",
        "\n",
        "        mask = counts > 0\n",
        "        importances[mask] /= counts[mask]\n",
        "\n",
        "        if np.sum(importances) > 0:\n",
        "            importances /= np.sum(importances)\n",
        "\n",
        "        self.feature_importances_ = importances\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if X.shape[1] != self.n_features:\n",
        "            raise ValueError(\n",
        "                f\"Number of features in prediction data ({X.shape[1]}) \"\n",
        "                f\"does not match training data ({self.n_features})\"\n",
        "            )\n",
        "\n",
        "        predictions = np.zeros((len(X), self.n_estimators))\n",
        "\n",
        "        for i, (tree, feature_subset) in enumerate(self.trees):\n",
        "            X_subset = X[:, feature_subset]\n",
        "            predictions[:, i] = tree.predict(X_subset)\n",
        "\n",
        "        return np.array([\n",
        "            Counter(row).most_common(1)[0][0]\n",
        "            for row in predictions\n",
        "        ])\n",
        "\n",
        "    def get_feature_importance_plot(self, feature_names=None):\n",
        "        if self.feature_importances_ is None:\n",
        "            raise ValueError(\"No feature importances available. Model must be trained first.\")\n",
        "\n",
        "        importances = self.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title('Feature Importances')\n",
        "\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Feature {i}' for i in range(len(importances))]\n",
        "\n",
        "        plt.bar(range(len(importances)),\n",
        "                importances[indices],\n",
        "                align='center')\n",
        "        plt.xticks(range(len(importances)),\n",
        "                   [feature_names[i] for i in indices],\n",
        "                   rotation=45)\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDX-gDLUS2iQ"
      },
      "source": [
        "## Features Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yrer3WRpS7X_"
      },
      "outputs": [],
      "source": [
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_engineered = df.copy()\n",
        "\n",
        "    df_engineered['balance_diff_orig'] = df['oldbalanceOrg'] - df['newbalanceOrig']\n",
        "    df_engineered['balance_diff_dest'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
        "\n",
        "    eps = 1e-10\n",
        "    df_engineered['balance_ratio_orig'] = (df['newbalanceOrig'] + eps) / (df['oldbalanceOrg'] + eps)\n",
        "    df_engineered['balance_ratio_dest'] = (df['newbalanceDest'] + eps) / (df['oldbalanceDest'] + eps)\n",
        "\n",
        "    df_engineered['error_balance_orig'] = df['amount'] - (df['oldbalanceOrg'] - df['newbalanceOrig'])\n",
        "    df_engineered['error_balance_dest'] = df['amount'] - (df['newbalanceDest'] - df['oldbalanceDest'])\n",
        "\n",
        "    df_engineered['amount_ratio_orig'] = df['amount'] / (df['oldbalanceOrg'] + eps)\n",
        "    df_engineered['amount_ratio_dest'] = df['amount'] / (df['oldbalanceDest'] + eps)\n",
        "\n",
        "    df_engineered['zero_balance_orig'] = ((df['oldbalanceOrg'] == 0) | (df['newbalanceOrig'] == 0)).astype(int)\n",
        "    df_engineered['zero_balance_dest'] = ((df['oldbalanceDest'] == 0) | (df['newbalanceDest'] == 0)).astype(int)\n",
        "    df_engineered['full_transfer'] = (df['newbalanceOrig'] == 0).astype(int)\n",
        "    df_engineered['balance_mismatch'] = (abs(df['oldbalanceOrg'] - df['newbalanceOrig']) != df['amount']).astype(int)\n",
        "\n",
        "    df_engineered['hour'] = df['step'] % 24\n",
        "    df_engineered['day'] = df['step'] // 24\n",
        "\n",
        "    df_engineered['dest_is_merchant'] = (df['nameDest'].str.startswith('M')).astype(int)\n",
        "\n",
        "    return df_engineered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfPsFs5jTIX-"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fZxOLxymTLF4"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df: pd.DataFrame, sampling_strategy: str, use_feature_engineering: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    if use_feature_engineering:\n",
        "        df = engineer_features(df)\n",
        "\n",
        "    base_features = ['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "                    'oldbalanceDest', 'newbalanceDest']\n",
        "\n",
        "    if use_feature_engineering:\n",
        "        additional_features = [\n",
        "            'balance_diff_orig', 'balance_diff_dest',\n",
        "            'balance_ratio_orig', 'balance_ratio_dest',\n",
        "            'error_balance_orig', 'error_balance_dest',\n",
        "            'amount_ratio_orig', 'amount_ratio_dest',\n",
        "            'zero_balance_orig', 'zero_balance_dest',\n",
        "            'full_transfer', 'balance_mismatch',\n",
        "            'hour', 'day', 'dest_is_merchant'\n",
        "        ]\n",
        "        features = base_features + additional_features\n",
        "    else:\n",
        "        features = base_features\n",
        "\n",
        "    categorical_features = ['type']\n",
        "    df_encoded = pd.get_dummies(df[features], columns=categorical_features)\n",
        "\n",
        "    X = df_encoded.values.astype(np.float64)\n",
        "    y = df['isFraud'].values.astype(np.int32)\n",
        "\n",
        "    for i in range(X.shape[1]):\n",
        "        mean_val = np.mean(X[:, i])\n",
        "        std_val = np.std(X[:, i])\n",
        "        if std_val != 0:\n",
        "            X[:, i] = (X[:, i] - mean_val) / std_val\n",
        "        else:\n",
        "            X[:, i] = X[:, i] - mean_val\n",
        "\n",
        "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    if sampling_strategy != 'none':\n",
        "        X, y = handle_imbalanced_data(X, y, sampling_strategy)\n",
        "\n",
        "    y = y.astype(np.int32)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def handle_imbalanced_data(X: np.ndarray, y: np.ndarray, strategy: str = 'combine') -> Tuple[np.ndarray, np.ndarray]:\n",
        "    print(f\"\\nOriginal dataset composition:\")\n",
        "    print(f\"Fraud transactions: {np.sum(y == 1)}\")\n",
        "    print(f\"Non-fraud transactions: {np.sum(y == 0)}\")\n",
        "    print(f\"Ratio fraud:non-fraud = 1:{np.sum(y == 0)/np.sum(y == 1):.0f}\")\n",
        "\n",
        "    y = y.astype(np.int32)\n",
        "\n",
        "    fraud_indices = np.where(y == 1)[0]\n",
        "    non_fraud_indices = np.where(y == 0)[0]\n",
        "\n",
        "    n_fraud = len(fraud_indices)\n",
        "    n_non_fraud = len(non_fraud_indices)\n",
        "\n",
        "    if strategy == 'undersample':\n",
        "        selected_non_fraud_indices = np.random.choice(\n",
        "            non_fraud_indices,\n",
        "            size=n_fraud,\n",
        "            replace=False\n",
        "        )\n",
        "        selected_indices = np.concatenate([fraud_indices, selected_non_fraud_indices])\n",
        "        return X[selected_indices], y[selected_indices]\n",
        "\n",
        "    elif strategy == 'oversample':\n",
        "        fraud_indices_oversampled = np.random.choice(\n",
        "            fraud_indices,\n",
        "            size=n_non_fraud,\n",
        "            replace=True\n",
        "        )\n",
        "        selected_indices = np.concatenate([fraud_indices_oversampled, non_fraud_indices])\n",
        "        return X[selected_indices], y[selected_indices]\n",
        "\n",
        "    elif strategy == 'combine':\n",
        "        target_size = int(n_non_fraud * 0.5)\n",
        "        selected_non_fraud_indices = np.random.choice(\n",
        "            non_fraud_indices,\n",
        "            size=target_size,\n",
        "            replace=False\n",
        "        )\n",
        "        fraud_indices_oversampled = np.random.choice(\n",
        "            fraud_indices,\n",
        "            size=target_size,\n",
        "            replace=True\n",
        "        )\n",
        "        selected_indices = np.concatenate([fraud_indices_oversampled, selected_non_fraud_indices])\n",
        "        return X[selected_indices], y[selected_indices]\n",
        "\n",
        "    elif strategy == 'smote':\n",
        "        X_fraud = X[fraud_indices]\n",
        "        n_synthetic = n_non_fraud - n_fraud\n",
        "        k = 5\n",
        "\n",
        "        synthetic_samples = []\n",
        "        for idx in range(len(X_fraud)):\n",
        "            distances = np.linalg.norm(X_fraud - X_fraud[idx], axis=1)\n",
        "            neighbor_indices = np.argsort(distances)[1:k+1]\n",
        "\n",
        "            n_synthetic_i = n_synthetic // len(X_fraud) + (1 if idx < n_synthetic % len(X_fraud) else 0)\n",
        "            for _ in range(n_synthetic_i):\n",
        "                neighbor_idx = np.random.choice(neighbor_indices)\n",
        "                diff = X_fraud[neighbor_idx] - X_fraud[idx]\n",
        "                synthetic_sample = X_fraud[idx] + np.random.random() * diff\n",
        "                synthetic_samples.append(synthetic_sample)\n",
        "\n",
        "        if synthetic_samples:\n",
        "            X_synthetic = np.vstack(synthetic_samples)\n",
        "            X = np.vstack([X, X_synthetic])\n",
        "            y = np.concatenate([y, np.ones(len(synthetic_samples))])\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZqBLG3TQF4"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Mz-_GHy_TS4Q"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    balanced_accuracy = (recall + specificity) / 2\n",
        "\n",
        "    g_mean = np.sqrt(recall * specificity) if (recall * specificity) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'specificity': specificity,\n",
        "        'balanced_accuracy': balanced_accuracy,\n",
        "        'g_mean': g_mean\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEX59BUxTb86"
      },
      "source": [
        "## Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Spu99JtwTXMR"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, title: str = \"Confusion Matrix\"):\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    thresh = confusion_matrix.max() / 2\n",
        "    for i, j in np.ndindex(confusion_matrix.shape):\n",
        "        plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    classes = ['Not Fraud', 'Fraud']\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSxetlP3Tj_L"
      },
      "source": [
        "## Plot ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3Ar56wgpTnm8"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(y_true: np.ndarray, y_prob: np.ndarray, title: str = \"ROC Curve\"):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11dRATZzXtZh"
      },
      "source": [
        "## Plot Features Important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ommpQQz1XzZB"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(model, feature_names: List[str], title: str = \"Feature Importance\"):\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.bar(range(len(importances)),\n",
        "            importances[indices],\n",
        "            color='royalblue')\n",
        "\n",
        "    plt.xticks(range(len(importances)),\n",
        "               [feature_names[i] for i in indices],\n",
        "               rotation=45,\n",
        "               ha='right')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Importance')\n",
        "\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLex0kyqTrdM"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkwAlyw2RS4k",
        "outputId": "9c62511e-3e21-4e8a-eed4-a84ca3625703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\n",
            "Preprocessing data...\n",
            "\n",
            "Original dataset composition:\n",
            "Fraud transactions: 8213\n",
            "Non-fraud transactions: 100000\n",
            "Ratio fraud:non-fraud = 1:12\n",
            "\n",
            "Splitting data...\n",
            "\n",
            "Training Decision Tree...\n"
          ]
        }
      ],
      "source": [
        "def sampling_dataset(df, sample_size):\n",
        "    isFraud = df[df['isFraud'] == 1]\n",
        "    nonFraud = df[df['isFraud'] == 0]\n",
        "\n",
        "    sampling_df = pd.concat([isFraud, nonFraud.sample(n=sample_size, random_state=2)])\n",
        "\n",
        "    return sampling_df.sample(frac=1, random_state=2).reset_index(drop=True)\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Dataset Fraud Detection/PS_20174392719_1491204439457_log.csv')\n",
        "    df = sampling_dataset(df, 100_000)\n",
        "\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    X, y = preprocess_data(df, sampling_strategy='smote')\n",
        "\n",
        "    print(\"\\nSplitting data...\")\n",
        "    indices = np.random.permutation(len(X))\n",
        "    train_size = int(0.8 * len(X))\n",
        "\n",
        "    X_train = X[indices[:train_size]]\n",
        "    y_train = y[indices[:train_size]]\n",
        "    X_test = X[indices[train_size:]]\n",
        "    y_test = y[indices[train_size:]]\n",
        "\n",
        "    print(\"\\nTraining Decision Tree...\")\n",
        "    dt = DecisionTree(max_depth=5, min_samples_split=5)\n",
        "    dt.fit(X_train, y_train)\n",
        "    dt_pred = dt.predict(X_test)\n",
        "\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "    rf = RandomForest(\n",
        "        n_estimators=20,\n",
        "        max_depth=5,\n",
        "        min_samples_split=5,\n",
        "        max_features='sqrt',\n",
        "        random_state=808\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    rf_pred = rf.predict(X_test)\n",
        "\n",
        "    with open('decision_tree_model.pkl', 'wb') as f:\n",
        "        pickle.dump(dt, f)\n",
        "\n",
        "    with open('random_forest_model.pkl', 'wb') as f:\n",
        "        pickle.dump(rf, f)\n",
        "\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    print(\"\\nDecision Tree:\")\n",
        "    dt_metrics = evaluate_model(y_test, dt_pred)\n",
        "    for metric, value in dt_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\nRandom Forest:\")\n",
        "    rf_metrics = evaluate_model(y_test, rf_pred)\n",
        "    for metric, value in rf_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plot_confusion_matrix(y_test, dt_pred, \"Decision Tree Confusion Matrix\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plot_confusion_matrix(y_test, rf_pred, \"Random Forest Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    feature_names = list(pd.get_dummies(\n",
        "        df[['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "            'oldbalanceDest', 'newbalanceDest']],\n",
        "        columns=['type']\n",
        "    ).columns)\n",
        "\n",
        "    with open('feature_names.pkl', 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plot_feature_importance(dt, feature_names, \"Decision Tree Feature Importance\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plot_feature_importance(rf, feature_names, \"Random Forest Feature Importance\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cH88VO0NR1BD",
        "TDX-gDLUS2iQ",
        "gzZqBLG3TQF4",
        "zEX59BUxTb86",
        "pSxetlP3Tj_L",
        "11dRATZzXtZh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
